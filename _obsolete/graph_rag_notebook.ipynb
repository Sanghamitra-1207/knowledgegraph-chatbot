{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b4eebbd6157a19",
   "metadata": {},
   "source": "CONNECT TO GALILEO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261248b635b75e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "aws_key = \"\"\n",
    "azure_key = \"\"\n",
    "llm = ChatOpenAI(\n",
    "    model=\"GPT-4o-2024-05-13\",\n",
    "    base_url=\"https://eu.aigw.galileo.roche.com/v1\",\n",
    "    api_key=azure_key,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9dc99f032d82",
   "metadata": {},
   "source": "TRANSFORM TEXT TO GRAPH"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df1f31621d1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "aws_key = \"\"\n",
    "azure_key = \"\"\n",
    "# Initialize LLM\n",
    "llm_grapher = ChatOpenAI(\n",
    "    model=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "    base_url=\"https://eu.aigw.galileo.roche.com/v1\",\n",
    "    api_key=azure_key,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Initialize LLMGraphTransformer\n",
    "graph_transformer = LLMGraphTransformer(llm=llm_grapher, ignore_tool_usage=True)\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "# loader = JSONLoader(file_path,\n",
    "#     jq_schema='.RecordList[].content',\n",
    "#     text_content=False)\n",
    "# pages = loader.load_and_split()\n",
    "pages = json.loads(Path(file_path).read_text(encoding=\"utf8\"))\n",
    "\n",
    "text_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "docs = text_splitter.create_documents(texts=[pages])\n",
    "\n",
    "\n",
    "lc_docs = []\n",
    "for doc in docs:\n",
    "    lc_docs.append(\n",
    "        Document(\n",
    "            page_content=doc.page_content.replace(\"\\n\", \"\"),\n",
    "            metadata={\"source\": file_path},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Clear the graph database\n",
    "# cypher = \"\"\"\n",
    "#   MATCH (n)\n",
    "#   DETACH DELETE n;\n",
    "# \"\"\"\n",
    "# graph.query(cypher)\n",
    "\n",
    "# Define allowed nodes and relationships\n",
    "allowed_nodes = [\"Paper\", \"Skillset\", \"Owner\"]\n",
    "allowed_relationships = [\"CREATED\", \"HAS_SKILLSET\", \"OWNS\"]\n",
    "\n",
    "# Transform documents into graph documents\n",
    "transformer = LLMGraphTransformer(\n",
    "    llm=llm_grapher,\n",
    "    # allowed_nodes=allowed_nodes,\n",
    "    # allowed_relationships=allowed_relationships,\n",
    "    node_properties=False,\n",
    "    relationship_properties=False,\n",
    "    ignore_tool_usage=True,\n",
    ")\n",
    "\n",
    "graph_documents = transformer.convert_to_graph_documents(lc_docs)\n",
    "print(graph_documents[0])\n",
    "# graph.add_graph_documents(graph_documents, include_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdddda4fecbfb78",
   "metadata": {},
   "source": "UPLOAD DATA TO NEO4J\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72f3d227bda5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://rkalvinpypoc.kau.roche.com:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"\"\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec491cdd444b2eb",
   "metadata": {},
   "source": "ASK"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f55fe3e457758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "aws_key = \"\"\n",
    "azure_key = \"\"\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    base_url=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "    api_key=aws_key,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the graph schema\n",
    "schema = graph.get_schema\n",
    "# question = \"search the persons who have know Bioinformatics and the Papers they Authored and return a comprehensive list with the name of the Author and the title of the paper next to it \"\n",
    "queries = [\"give me persons who know Bioinformatics\", \"now list their Papers\"]\n",
    "question = \"DAN | South East Asian Thoracic Society (SEATS) Annual Meeting has been created by who\"\n",
    "# Set up the QA chain\n",
    "template = \"\"\"\n",
    "Instructions:\n",
    "Use only relationship types and properties provided in schema.\n",
    "Do not use other relationship types or properties that are not provided.\n",
    "Use only data as your answer.\n",
    "\n",
    "\n",
    "schema:\n",
    "{schema}\n",
    "\n",
    "Note:\n",
    "Do not include explanations or apologies in your answers.\n",
    "Do not answer questions that ask anything other than creating Cypher statements.\n",
    "Do not include any text other than generated Cypher statements.\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Task: Generate a Cypher statement to query the graph database.\n",
    "\"\"\"\n",
    "question_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"schema\", \"question\"]\n",
    ")\n",
    "qa = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=question_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    output_parser=StrOutputParser(),\n",
    "    runnable=RunnablePassthrough(),\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "res = qa.invoke(question)[\"intermediate_steps\"][1][\"context\"]\n",
    "\n",
    "answer_template = \"\"\"\n",
    "Instructions:\n",
    "Use only the Query Result as the basis for your answer\n",
    "Start your answer with the following line: \"The answer to your question is:\"\n",
    "\n",
    "Query Result:\n",
    "{res}\n",
    "\n",
    "Note:\n",
    "Do not include explanations or apologies in your answers.\n",
    "\n",
    "Task: Provide the Query Result as a human readible bulletpoint list.\n",
    "\"\"\"\n",
    "\n",
    "# answer_prompt = PromptTemplate(\n",
    "# template=answer_template,\n",
    "# input_variables=[\"question\", \"res\"]\n",
    "# )\n",
    "prompt = PromptTemplate.from_template(template=answer_template)\n",
    "prompt_formatted_str: str = prompt.format(question=question, res=res)\n",
    "result = llm.invoke(prompt_formatted_str)\n",
    "print(str(result.pretty_print()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
