{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "# AWS credentials for the account with graph data\n",
    "environ[\"AWS_DEFAULT_REGION\"] = \"eu-central-1\"\n",
    "\n",
    "# galileo\n",
    "aws_key = \"\"\n",
    "azure_key = \"\"\n",
    "\n",
    "environ[\"AWS_ACCESS_KEY_ID\"]=\"\"\n",
    "environ[\"AWS_SECRET_ACCESS_KEY\"]=\"\"\n",
    "environ[\"AWS_SESSION_TOKEN\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537add92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.8.1.zip#subdirectory=lexical-graph\n",
      "  Using cached https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.8.1.zip\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting anthropic-bedrock==0.8.0\n",
      "  Using cached anthropic_bedrock-0.8.0-py3-none-any.whl (820 kB)\n",
      "Collecting python-dotenv==1.0.1\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: botocore>=1.36.1 in ./.venv/lib/python3.10/site-packages (from graphrag-toolkit-lexical-graph==3.8.1) (1.36.1)\n",
      "Collecting smart-open==7.1.0\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting llama-index-embeddings-bedrock==0.5.0\n",
      "  Using cached llama_index_embeddings_bedrock-0.5.0-py3-none-any.whl (5.6 kB)\n",
      "Collecting lru-dict==1.3.0\n",
      "  Using cached lru_dict-1.3.0-cp310-cp310-macosx_11_0_arm64.whl (11 kB)\n",
      "Collecting pipe==2.2\n",
      "  Using cached pipe-2.2-py3-none-any.whl (9.7 kB)\n",
      "Collecting llama-index-llms-anthropic==0.6.19\n",
      "  Using cached llama_index_llms_anthropic-0.6.19-py3-none-any.whl (12 kB)\n",
      "Collecting llama-index-core==0.12.37\n",
      "  Using cached llama_index_core-0.12.37-py3-none-any.whl (7.7 MB)\n",
      "Requirement already satisfied: boto3>=1.36.1 in ./.venv/lib/python3.10/site-packages (from graphrag-toolkit-lexical-graph==3.8.1) (1.36.1)\n",
      "Collecting llama-index-llms-bedrock-converse==0.6.0\n",
      "  Using cached llama_index_llms_bedrock_converse-0.6.0-py3-none-any.whl (14 kB)\n",
      "Collecting spacy==3.7.5\n",
      "  Using cached spacy-3.7.5-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
      "Collecting tfidf-matcher==0.3.0\n",
      "  Using cached tfidf_matcher-0.3.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting json2xml==5.0.5\n",
      "  Using cached json2xml-5.0.5-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (1.9.0)\n",
      "Collecting tokenizers>=0.13.0\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (4.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (2.11.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (4.13.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (1.3.1)\n",
      "Requirement already satisfied: defusedxml in ./.venv/lib/python3.10/site-packages (from json2xml==5.0.5->graphrag-toolkit-lexical-graph==3.8.1) (0.7.1)\n",
      "Requirement already satisfied: urllib3 in ./.venv/lib/python3.10/site-packages (from json2xml==5.0.5->graphrag-toolkit-lexical-graph==3.8.1) (2.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.9.1)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.4.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2025.5.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.6.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.9.0)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.21.0)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.2.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2.2.6)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.6.7)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.9.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (9.1.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2.0.41)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.17.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2.32.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.0.8)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (11.2.1)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2.1.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.10/site-packages (from llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.12.12)\n",
      "Collecting aioboto3<14.0.0,>=13.1.1\n",
      "  Using cached aioboto3-13.4.0-py3-none-any.whl (34 kB)\n",
      "Collecting anthropic[bedrock,vertex]>=0.52.0\n",
      "  Using cached anthropic-0.54.0-py3-none-any.whl (288 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.13-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Using cached thinc-8.2.5-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy==3.7.5->graphrag-toolkit-lexical-graph==3.8.1) (24.2)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.10-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy==3.7.5->graphrag-toolkit-lexical-graph==3.8.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy==3.7.5->graphrag-toolkit-lexical-graph==3.8.1) (63.2.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.0-cp310-cp310-macosx_12_0_arm64.whl (10.7 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.venv/lib/python3.10/site-packages (from boto3>=1.36.1->graphrag-toolkit-lexical-graph==3.8.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in ./.venv/lib/python3.10/site-packages (from boto3>=1.36.1->graphrag-toolkit-lexical-graph==3.8.1) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.venv/lib/python3.10/site-packages (from botocore>=1.36.1->graphrag-toolkit-lexical-graph==3.8.1) (2.9.0.post0)\n",
      "Collecting aiofiles>=23.2.1\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Collecting aiobotocore[boto3]==2.18.0\n",
      "  Using cached aiobotocore-2.18.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in ./.venv/lib/python3.10/site-packages (from aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock==0.5.0->graphrag-toolkit-lexical-graph==3.8.1) (6.4.4)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1\n",
      "  Using cached aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (25.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.20.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.3.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from anthropic[bedrock,vertex]>=0.52.0->llama-index-llms-anthropic==0.6.19->graphrag-toolkit-lexical-graph==3.8.1) (0.10.0)\n",
      "Collecting google-auth[requests]<3,>=2\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (3.10)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.7.3)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (4.3.8)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (0.16.0)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (2.33.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.36.1->graphrag-toolkit-lexical-graph==3.8.1) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.2.3)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (1.1.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy==3.7.5->graphrag-toolkit-lexical-graph==3.8.1) (3.0.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting scipy>=1.8.0\n",
      "  Using cached scipy-1.15.3-cp310-cp310-macosx_12_0_arm64.whl (30.1 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic-bedrock==0.8.0->graphrag-toolkit-lexical-graph==3.8.1) (3.18.0)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5->graphrag-toolkit-lexical-graph==3.8.1) (2.19.1)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core==0.12.37->graphrag-toolkit-lexical-graph==3.8.1) (0.4.6)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: graphrag-toolkit-lexical-graph\n",
      "  Building wheel for graphrag-toolkit-lexical-graph (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for graphrag-toolkit-lexical-graph: filename=graphrag_toolkit_lexical_graph-3.8.1-py3-none-any.whl size=374156 sha256=a465242ef0f1fe9677d4823db8be792321c12668fc03d9276760da927ea379a7\n",
      "  Stored in directory: /private/var/folders/dw/jnv3zzdd3873z1t_d17pnc2h0000gp/T/pip-ephem-wheel-cache-5ppv5974/wheels/d4/8c/3e/5b5cac8396ef1997d02a1a91e7f2ca71e10cb76613162cf881\n",
      "Successfully built graphrag-toolkit-lexical-graph\n",
      "Installing collected packages: pytz, cymem, wasabi, tzdata, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, shellingham, python-dotenv, pyasn1, pipe, numpy, murmurhash, mdurl, marisa-trie, lru-dict, json2xml, hf-xet, cloudpathlib, catalogue, cachetools, aioitertools, aiofiles, srsly, scipy, rsa, pyasn1-modules, preshed, pandas, markdown-it-py, language-data, huggingface-hub, blis, tokenizers, scikit-learn, rich, langcodes, google-auth, confection, typer, thinc, tfidf-matcher, llama-index-core, anthropic, aiobotocore, weasel, anthropic-bedrock, spacy, llama-index-llms-anthropic, aioboto3, llama-index-llms-bedrock-converse, llama-index-embeddings-bedrock, graphrag-toolkit-lexical-graph\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.12.42\n",
      "    Uninstalling llama-index-core-0.12.42:\n",
      "      Successfully uninstalled llama-index-core-0.12.42\n",
      "Successfully installed aioboto3-13.4.0 aiobotocore-2.18.0 aiofiles-24.1.0 aioitertools-0.12.0 anthropic-0.54.0 anthropic-bedrock-0.8.0 blis-0.7.11 cachetools-5.5.2 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 google-auth-2.40.3 graphrag-toolkit-lexical-graph-3.8.1 hf-xet-1.1.3 huggingface-hub-0.33.0 json2xml-5.0.5 langcodes-3.5.0 language-data-1.3.0 llama-index-core-0.12.37 llama-index-embeddings-bedrock-0.5.0 llama-index-llms-anthropic-0.6.19 llama-index-llms-bedrock-converse-0.6.0 lru-dict-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-1.26.4 pandas-2.3.0 pipe-2.2 preshed-3.0.10 pyasn1-0.6.1 pyasn1-modules-0.4.2 python-dotenv-1.0.1 pytz-2025.2 rich-14.0.0 rsa-4.9.1 scikit-learn-1.7.0 scipy-1.15.3 shellingham-1.5.4 smart-open-7.1.0 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 tfidf-matcher-0.3.0 thinc-8.2.5 threadpoolctl-3.6.0 tokenizers-0.21.1 typer-0.16.0 tzdata-2025.2 wasabi-1.1.3 weasel-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.8.1.zip#subdirectory=lexical-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b70acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f78c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = \"neptune-graph://g-76pgb4ql43\"\n",
    "vector_store = \"aoss://https://r8mamuo7hin4k3xihz9a.eu-central-1.aoss.amazonaws.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c204f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-web in ./.venv/lib/python3.10/site-packages (0.4.2)\n",
      "Collecting llama-index-llms-openllm\n",
      "  Using cached llama_index_llms_openllm-0.4.2-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (4.13.4)\n",
      "Requirement already satisfied: spider-client<0.0.28,>=0.0.27 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.0.27)\n",
      "Requirement already satisfied: requests<3,>=2.31.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (2.32.4)\n",
      "Requirement already satisfied: html2text<2025,>=2024.2.26 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (2024.2.26)\n",
      "Requirement already satisfied: aiohttp<4,>=3.9.1 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (3.12.12)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.12.42)\n",
      "Requirement already satisfied: lxml>=5.4.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (5.4.0)\n",
      "Requirement already satisfied: oxylabs>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (2.0.0)\n",
      "Requirement already satisfied: defusedxml<0.8,>=0.7.1 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.7.1)\n",
      "Requirement already satisfied: httpx>=0.28.1 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.28.1)\n",
      "Requirement already satisfied: lxml-html-clean>=0.4.2 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.4.2)\n",
      "Requirement already satisfied: playwright<2.0,>=1.30 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (1.52.0)\n",
      "Requirement already satisfied: selenium<5,>=4.17.2 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (4.33.0)\n",
      "Requirement already satisfied: urllib3>=1.1.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (2.4.0)\n",
      "Requirement already satisfied: chromedriver-autoinstaller<0.7,>=0.6.3 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.6.4)\n",
      "Requirement already satisfied: newspaper3k<0.3,>=0.2.8 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (0.2.8)\n",
      "Requirement already satisfied: markdownify>=1.1.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-web) (1.1.0)\n",
      "Collecting llama-index-llms-openai-like<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai_like-0.4.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.30-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (6.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (1.20.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->llama-index-readers-web) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-web) (4.13.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-web) (2.7)\n",
      "Requirement already satisfied: packaging>=23.1 in ./.venv/lib/python3.10/site-packages (from chromedriver-autoinstaller<0.7,>=0.6.3->llama-index-readers-web) (24.2)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx>=0.28.1->llama-index-readers-web) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx>=0.28.1->llama-index-readers-web) (1.0.9)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx>=0.28.1->llama-index-readers-web) (2025.4.26)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx>=0.28.1->llama-index-readers-web) (4.9.0)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.28.1->llama-index-readers-web) (0.16.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (6.0.2)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.2.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (0.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.2.18)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.6.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.17.2)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (0.6.7)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (4.67.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (3.9.1)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (2.1.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (2.2.6)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (0.9.0)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (0.21.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (9.1.2)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (3.4.2)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (2.11.5)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.0.8)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (11.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (2025.5.1)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (2.0.41)\n",
      "Collecting transformers<5,>=4.37.0\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six<2,>=1.15 in ./.venv/lib/python3.10/site-packages (from markdownify>=1.1.0->llama-index-readers-web) (1.17.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.0.4)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (5.3.0)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.35.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in ./.venv/lib/python3.10/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.3)\n",
      "Collecting bentoml==1.4.8\n",
      "  Using cached bentoml-1.4.8-py3-none-any.whl (1.2 MB)\n",
      "Collecting uv\n",
      "  Downloading uv-0.7.13-py3-none-macosx_11_0_arm64.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting pip-requirements-parser\n",
      "  Using cached pip_requirements_parser-32.0.1-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (7.0.0)\n",
      "Collecting typer\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Collecting openai==1.73.0\n",
      "  Using cached openai-1.73.0-py3-none-any.whl (644 kB)\n",
      "Collecting dulwich\n",
      "  Using cached dulwich-0.22.8-cp310-cp310-macosx_11_0_arm64.whl (925 kB)\n",
      "Collecting questionary\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Collecting nvidia-ml-py\n",
      "  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Collecting pyaml\n",
      "  Using cached pyaml-25.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting pathspec\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting tomli-w\n",
      "  Using cached tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Collecting python-json-logger\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting cattrs<23.2.0,>=22.1.0\n",
      "  Using cached cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
      "Collecting python-multipart\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Collecting opentelemetry-sdk~=1.20\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-semantic-conventions~=0.41b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-util-http~=0.41b0\n",
      "  Using cached opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
      "Collecting prometheus-client>=0.10.0\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Collecting kantoku>=0.18.3\n",
      "  Using cached kantoku-0.18.3-py3-none-any.whl (118 kB)\n",
      "Collecting simple-di>=0.1.4\n",
      "  Using cached simple_di-0.1.5-py3-none-any.whl (9.8 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
      "Collecting uvicorn>=0.22.0\n",
      "  Using cached uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Collecting opentelemetry-instrumentation~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
      "Collecting watchfiles>=0.15.0\n",
      "  Using cached watchfiles-1.0.5-cp310-cp310-macosx_11_0_arm64.whl (395 kB)\n",
      "Collecting python-dotenv>=1.0.1\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (8.2.1)\n",
      "Collecting httpx-ws>=0.6.0\n",
      "  Using cached httpx_ws-0.7.2-py3-none-any.whl (14 kB)\n",
      "Collecting cloudpickle>=2.0.0\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting a2wsgi>=1.10.7\n",
      "  Using cached a2wsgi-1.10.8-py3-none-any.whl (17 kB)\n",
      "Collecting rich>=11.2.0\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-api~=1.20\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Collecting fs\n",
      "  Using cached fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.1 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (3.1.6)\n",
      "Collecting click-option-group\n",
      "  Using cached click_option_group-0.5.7-py3-none-any.whl (11 kB)\n",
      "Collecting starlette>=0.24.0\n",
      "  Using cached starlette-0.47.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-instrumentation-aiohttp-client~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_aiohttp_client-0.55b1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (0.10.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in ./.venv/lib/python3.10/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in ./.venv/lib/python3.10/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (3.2.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.31.0->llama-index-readers-web) (3.4.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in ./.venv/lib/python3.10/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (1.8.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in ./.venv/lib/python3.10/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (0.12.2)\n",
      "Requirement already satisfied: trio~=0.30.0 in ./.venv/lib/python3.10/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (0.30.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.28.1->llama-index-readers-web) (1.3.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (4.3.8)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-web) (1.7.3)\n",
      "Requirement already satisfied: sgmllib3k in ./.venv/lib/python3.10/site-packages (from feedparser>=5.2.1->newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (1.0.0)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai-0.4.4-py3-none-any.whl (25 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.3-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.2-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.1-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.0-py3-none-any.whl (24 kB)\n",
      "INFO: pip is looking at multiple versions of jieba3k to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Using cached jieba3k-0.35.1.zip (7.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "INFO: pip is looking at multiple versions of greenlet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting greenlet<4.0.0,>=3.1.1\n",
      "  Using cached greenlet-3.2.3-cp310-cp310-macosx_11_0_universal2.whl (268 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "INFO: pip is looking at multiple versions of frozenlist to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
      "INFO: pip is looking at multiple versions of filetype to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting filetype<2,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of feedparser to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting feedparser>=5.2.1\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "INFO: pip is looking at multiple versions of feedfinder2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Using cached feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of dirtyjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dirtyjson<2,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of deprecated to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting deprecated>=1.2.9.3\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "INFO: pip is looking at multiple versions of cssselect to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cssselect>=0.9.2\n",
      "  Using cached cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "INFO: pip is looking at multiple versions of charset-normalizer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl (201 kB)\n",
      "INFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting certifi\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "INFO: pip is looking at multiple versions of banks to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting banks<3,>=2.0.0\n",
      "  Using cached banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "INFO: pip is looking at multiple versions of async-timeout to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "INFO: pip is looking at multiple versions of anyio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "INFO: pip is looking at multiple versions of aiosqlite to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosqlite\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of aiosignal to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "INFO: pip is looking at multiple versions of aiohappyeyeballs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting urllib3>=1.1.0\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "INFO: pip is looking at multiple versions of spider-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spider-client<0.0.28,>=0.0.27\n",
      "  Using cached spider-client-0.0.27.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of selenium to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting selenium<5,>=4.17.2\n",
      "  Using cached selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3,>=2.31.0\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "INFO: pip is looking at multiple versions of playwright to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting playwright<2.0,>=1.30\n",
      "  Using cached playwright-1.52.0-py3-none-macosx_11_0_arm64.whl (38.0 MB)\n",
      "INFO: pip is looking at multiple versions of oxylabs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oxylabs>=2.0.0\n",
      "  Using cached oxylabs-2.0.0-py3-none-any.whl (34 kB)\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of bentoml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.29-py3-none-any.whl (31 kB)\n",
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.28-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.27-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.26-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.25-py3-none-any.whl (31 kB)\n",
      "Collecting openai==1.70.0\n",
      "  Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.24-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.23-py3-none-any.whl (30 kB)\n",
      "Collecting bentoml==1.4.7\n",
      "  Using cached bentoml-1.4.7-py3-none-any.whl (1.1 MB)\n",
      "Collecting openai==1.69.0\n",
      "  Using cached openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "INFO: pip is looking at multiple versions of bentoml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.22-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.21-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.20-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.65.5\n",
      "  Using cached openai-1.65.5-py3-none-any.whl (474 kB)\n",
      "Collecting bentoml==1.4.3\n",
      "  Using cached bentoml-1.4.3-py3-none-any.whl (1.1 MB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.19-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.61.1\n",
      "  Using cached openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "Collecting bentoml==1.4.0a2\n",
      "  Using cached bentoml-1.4.0a2-py3-none-any.whl (1.1 MB)\n",
      "Collecting inflection\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting deepmerge\n",
      "  Using cached deepmerge-2.0-py3-none-any.whl (13 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.18-py3-none-any.whl (29 kB)\n",
      "Collecting bentoml\n",
      "  Using cached bentoml-1.4.15-py3-none-any.whl (1.2 MB)\n",
      "Collecting openai==1.61.0\n",
      "  Using cached openai-1.61.0-py3-none-any.whl (460 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.17-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.59.3\n",
      "  Using cached openai-1.59.3-py3-none-any.whl (454 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.16-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.57.4\n",
      "  Using cached openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.15-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.55.0\n",
      "  Using cached openai-1.55.0-py3-none-any.whl (389 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.14-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.52.0\n",
      "  Using cached openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.13-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.51.2\n",
      "  Using cached openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.12-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.11-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.47.0\n",
      "  Using cached openai-1.47.0-py3-none-any.whl (375 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.10-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.41.0\n",
      "  Using cached openai-1.41.0-py3-none-any.whl (362 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.9-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.38.0\n",
      "  Using cached openai-1.38.0-py3-none-any.whl (335 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.8-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.37.1\n",
      "  Using cached openai-1.37.1-py3-none-any.whl (337 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.7-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.6-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.35.12\n",
      "  Using cached openai-1.35.12-py3-none-any.whl (328 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.4-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.3-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.2-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.1-py3-none-any.whl (29 kB)\n",
      "  Using cached openllm-0.6.0-py3-none-any.whl (29 kB)\n",
      "INFO: pip is looking at multiple versions of tinysegmenter to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tinysegmenter==0.3\n",
      "  Using cached tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of newspaper3k to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting newspaper3k<0.3,>=0.2.8\n",
      "  Using cached newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "INFO: pip is looking at multiple versions of markdownify to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting markdownify>=1.1.0\n",
      "  Using cached markdownify-1.1.0-py3-none-any.whl (13 kB)\n",
      "INFO: pip is looking at multiple versions of lxml-html-clean to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml-html-clean>=0.4.2\n",
      "  Using cached lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml>=5.4.0\n",
      "  Using cached lxml-5.4.0-cp310-cp310-macosx_10_9_universal2.whl (8.1 MB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai-like to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of llama-index-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-core<0.13,>=0.12.0\n",
      "  Using cached llama_index_core-0.12.42-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.41-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.40-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.39-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.38-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.37-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.36-py3-none-any.whl (7.7 MB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai-like to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of llama-index-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached llama_index_core-0.12.35-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.34.post1-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.34-py3-none-any.whl (7.6 MB)\n",
      "  Using cached llama_index_core-0.12.33.post1-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.33-py3-none-any.whl (814 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_index_core-0.12.32-py3-none-any.whl (809 kB)\n",
      "  Using cached llama_index_core-0.12.31-py3-none-any.whl (808 kB)\n",
      "  Using cached llama_index_core-0.12.30-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.29-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.28-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.27-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.26-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.25-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.24.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.24-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.22-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.21-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.20-py3-none-any.whl (790 kB)\n",
      "  Using cached llama_index_core-0.12.19-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.18-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.17-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.16.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.16-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.15-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.14-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.13-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.12-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.11-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.10-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.9-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.8-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.7-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.6-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.5-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.4-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.3-py3-none-any.whl (1.6 MB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.0\n",
      "  Using cached llama_index_core-0.12.2-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.0-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: openai<2,>=1.81.0 in ./.venv/lib/python3.10/site-packages (from llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-openllm) (1.86.0)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of tinysegmenter to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of newspaper3k to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of markdownify to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of lxml-html-clean to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "  Using cached httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "  Using cached httpcore-1.0.1-py3-none-any.whl (76 kB)\n",
      "  Using cached httpcore-1.0.0-py3-none-any.whl (76 kB)\n",
      "INFO: pip is looking at multiple versions of httpx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpx>=0.28.1\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of httpx to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of html2text to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting html2text<2025,>=2024.2.26\n",
      "  Using cached html2text-2024.2.26.tar.gz (56 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of html2text to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of defusedxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting defusedxml<0.8,>=0.7.1\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of defusedxml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of chromedriver-autoinstaller to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chromedriver-autoinstaller<0.7,>=0.6.3\n",
      "  Using cached chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
      "  Using cached chromedriver_autoinstaller-0.6.3-py3-none-any.whl (7.6 kB)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
      "    with self.main_context():\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/cli/command_context.py\", line 19, in main_context\n",
      "    with self._main_context:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 576, in __exit__\n",
      "    raise exc_details[1]\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/utils/temp_dir.py\", line 70, in tempdir_registry\n",
      "    yield _tempdir_registry\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 561, in __exit__\n",
      "    if cb(*exc_details):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/utils/temp_dir.py\", line 31, in global_tempdir_manager\n",
      "    with ExitStack() as stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 576, in __exit__\n",
      "    raise exc_details[1]\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 561, in __exit__\n",
      "    if cb(*exc_details):\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/utils/temp_dir.py\", line 156, in __exit__\n",
      "    self.cleanup()\n",
      "  File \"/Users/kryjanm/projects/graph/graph-rag/.venv/lib/python3.10/site-packages/pip/_internal/utils/temp_dir.py\", line 171, in cleanup\n",
      "    if not os.path.exists(self._path):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-readers-web llama-index-llms-openllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cf9fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-readers-json\n",
      "  Using cached llama_index_readers_json-0.3.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-json) (0.12.42)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (6.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.2.18)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.32.4)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.6.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2025.5.1)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.12.12)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.11.5)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.0.8)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (11.2.1)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.17.2)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (4.13.2)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.28.1)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (4.67.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.9.1)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.21.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (9.1.2)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.2.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.2.6)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.9.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.4.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.9.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.20.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (6.4.4)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.1.6)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.7.3)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (4.3.8)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2024.11.6)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (8.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.26.1)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (24.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (1.3.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-json) (3.0.2)\n",
      "Installing collected packages: llama-index-readers-json\n",
      "Successfully installed llama-index-readers-json-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-readers-json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379971e0",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ffa048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-openai-like\n",
      "  Using cached llama_index_embeddings_openai_like-0.1.1-py3-none-any.whl (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0\n",
      "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./.venv/lib/python3.10/site-packages (from llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.86.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in ./.venv/lib/python3.10/site-packages (from llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.12.42)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.6.7)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (11.2.1)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.2.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.17.2)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.4.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.9.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.1.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (9.1.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.2.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.2.18)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (4.67.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.12.12)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.21.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.32.4)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (4.13.2)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2025.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.9.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (6.0.2)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.0.41)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.11.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (6.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.20.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.6.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.3.0)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.7.3)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (4.3.8)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.1.6)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2024.11.6)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.5.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (24.2)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-embeddings-openai-like) (3.0.2)\n",
      "Installing collected packages: llama-index-embeddings-openai, llama-index-embeddings-openai-like\n",
      "Successfully installed llama-index-embeddings-openai-0.3.1 llama-index-embeddings-openai-like-0.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-embeddings-openai-like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8289bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-openllm\n",
      "  Using cached llama_index_llms_openllm-0.4.2-py3-none-any.whl (3.5 kB)\n",
      "Collecting llama-index-llms-openai-like<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai_like-0.4.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in ./.venv/lib/python3.10/site-packages (from llama-index-llms-openllm) (0.12.37)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.30-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.12.12)\n",
      "Requirement already satisfied: aiosqlite in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.21.0)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.0.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (9.1.2)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.1.2)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.17.2)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.2.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.11.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.2.18)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (6.0.2)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.6.7)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.9.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.9.0)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.0.41)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.6.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (11.2.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.32.4)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.4.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (4.67.1)\n",
      "Collecting transformers<5,>=4.37.0\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
      "Collecting questionary\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Collecting openai==1.73.0\n",
      "  Using cached openai-1.73.0-py3-none-any.whl (644 kB)\n",
      "Requirement already satisfied: hf-xet in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (1.1.3)\n",
      "Requirement already satisfied: typer in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (0.16.0)\n",
      "Collecting nvidia-ml-py\n",
      "  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (0.33.0)\n",
      "Collecting pyaml\n",
      "  Using cached pyaml-25.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting uv\n",
      "  Using cached uv-0.7.13-py3-none-macosx_11_0_arm64.whl (15.8 MB)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting bentoml==1.4.8\n",
      "  Using cached bentoml-1.4.8-py3-none-any.whl (1.2 MB)\n",
      "Collecting dulwich\n",
      "  Using cached dulwich-0.22.8-cp310-cp310-macosx_11_0_arm64.whl (925 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (7.0.0)\n",
      "Collecting pip-requirements-parser\n",
      "  Using cached pip_requirements_parser-32.0.1-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: attrs in ./.venv/lib/python3.10/site-packages (from openllm>=0.6.0->llama-index-llms-openllm) (25.3.0)\n",
      "Collecting opentelemetry-instrumentation-aiohttp-client~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_aiohttp_client-0.55b1-py3-none-any.whl (12 kB)\n",
      "Collecting opentelemetry-semantic-conventions~=0.41b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-instrumentation~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
      "Collecting cloudpickle>=2.0.0\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting opentelemetry-util-http~=0.41b0\n",
      "  Using cached opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (24.2)\n",
      "Collecting click-option-group\n",
      "  Using cached click_option_group-0.5.7-py3-none-any.whl (11 kB)\n",
      "Collecting python-json-logger\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (8.2.1)\n",
      "Collecting prometheus-client>=0.10.0\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.1 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (3.1.6)\n",
      "Collecting opentelemetry-sdk~=1.20\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (1.0.1)\n",
      "Collecting starlette>=0.24.0\n",
      "  Using cached starlette-0.47.0-py3-none-any.whl (72 kB)\n",
      "Collecting a2wsgi>=1.10.7\n",
      "  Using cached a2wsgi-1.10.8-py3-none-any.whl (17 kB)\n",
      "Collecting tomli-w\n",
      "  Using cached tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Collecting pathspec\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting uvicorn>=0.22.0\n",
      "  Using cached uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Collecting opentelemetry-api~=1.20\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: rich>=11.2.0 in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (14.0.0)\n",
      "Collecting opentelemetry-instrumentation-asgi~=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
      "Collecting python-multipart\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Collecting watchfiles>=0.15.0\n",
      "  Using cached watchfiles-1.0.5-cp310-cp310-macosx_11_0_arm64.whl (395 kB)\n",
      "Collecting fs\n",
      "  Using cached fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.10/site-packages (from bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (2.9.0.post0)\n",
      "Collecting kantoku>=0.18.3\n",
      "  Using cached kantoku-0.18.3-py3-none-any.whl (118 kB)\n",
      "Collecting cattrs<23.2.0,>=22.1.0\n",
      "  Using cached cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
      "Collecting httpx-ws>=0.6.0\n",
      "  Using cached httpx_ws-0.7.2-py3-none-any.whl (14 kB)\n",
      "Collecting simple-di>=0.1.4\n",
      "  Using cached simple_di-0.1.5-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (0.10.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (6.4.4)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (5.0.1)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (4.3.8)\n",
      "Requirement already satisfied: griffe in ./.venv/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.7.3)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.0.9)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2025.4.26)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.16.0)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0\n",
      "  Using cached llama_index_llms_openai-0.4.4-py3-none-any.whl (25 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.3-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.2-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.1-py3-none-any.whl (24 kB)\n",
      "  Using cached llama_index_llms_openai-0.4.0-py3-none-any.whl (24 kB)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "  Using cached httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "  Using cached httpcore-1.0.1-py3-none-any.whl (76 kB)\n",
      "  Using cached httpcore-1.0.0-py3-none-any.whl (76 kB)\n",
      "INFO: pip is looking at multiple versions of httpx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpx\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "INFO: pip is looking at multiple versions of filetype to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting filetype<2,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of dirtyjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dirtyjson<2,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of deprecated to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting deprecated>=1.2.9.3\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "INFO: pip is looking at multiple versions of banks to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting banks<3,>=2.0.0\n",
      "  Using cached banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "INFO: pip is looking at multiple versions of aiosqlite to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosqlite\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of aiohttp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohttp<4,>=3.8.6\n",
      "  Using cached aiohttp-3.12.12-cp310-cp310-macosx_11_0_arm64.whl (466 kB)\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of bentoml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.29-py3-none-any.whl (31 kB)\n",
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.28-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.27-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.26-py3-none-any.whl (31 kB)\n",
      "  Using cached openllm-0.6.25-py3-none-any.whl (31 kB)\n",
      "Collecting openai==1.70.0\n",
      "  Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.24-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.23-py3-none-any.whl (30 kB)\n",
      "Collecting bentoml==1.4.7\n",
      "  Using cached bentoml-1.4.7-py3-none-any.whl (1.1 MB)\n",
      "Collecting openai==1.69.0\n",
      "  Using cached openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "INFO: pip is looking at multiple versions of bentoml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of openllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.22-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.21-py3-none-any.whl (30 kB)\n",
      "  Using cached openllm-0.6.20-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.65.5\n",
      "  Using cached openai-1.65.5-py3-none-any.whl (474 kB)\n",
      "Collecting bentoml==1.4.3\n",
      "  Using cached bentoml-1.4.3-py3-none-any.whl (1.1 MB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.19-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.61.1\n",
      "  Using cached openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "Collecting bentoml==1.4.0a2\n",
      "  Using cached bentoml-1.4.0a2-py3-none-any.whl (1.1 MB)\n",
      "Collecting deepmerge\n",
      "  Using cached deepmerge-2.0-py3-none-any.whl (13 kB)\n",
      "Collecting inflection\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.18-py3-none-any.whl (29 kB)\n",
      "Collecting bentoml\n",
      "  Using cached bentoml-1.4.15-py3-none-any.whl (1.2 MB)\n",
      "Collecting openai==1.61.0\n",
      "  Using cached openai-1.61.0-py3-none-any.whl (460 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.17-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.59.3\n",
      "  Using cached openai-1.59.3-py3-none-any.whl (454 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.16-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.57.4\n",
      "  Using cached openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.15-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.55.0\n",
      "  Using cached openai-1.55.0-py3-none-any.whl (389 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.14-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.52.0\n",
      "  Using cached openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.13-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.51.2\n",
      "  Using cached openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.12-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.11-py3-none-any.whl (29 kB)\n",
      "Collecting openai==1.47.0\n",
      "  Using cached openai-1.47.0-py3-none-any.whl (375 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.10-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.41.0\n",
      "  Using cached openai-1.41.0-py3-none-any.whl (362 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.9-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.38.0\n",
      "  Using cached openai-1.38.0-py3-none-any.whl (335 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.8-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.37.1\n",
      "  Using cached openai-1.37.1-py3-none-any.whl (337 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.7-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.6-py3-none-any.whl (28 kB)\n",
      "  Using cached openllm-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.35.12\n",
      "  Using cached openai-1.35.12-py3-none-any.whl (328 kB)\n",
      "Collecting openllm>=0.6.0\n",
      "  Using cached openllm-0.6.4-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.3-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.2-py3-none-any.whl (27 kB)\n",
      "  Using cached openllm-0.6.1-py3-none-any.whl (29 kB)\n",
      "  Using cached openllm-0.6.0-py3-none-any.whl (29 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai-like to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of llama-index-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-core<0.13,>=0.12.0\n",
      "  Using cached llama_index_core-0.12.42-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.41-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.40-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.39-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.38-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.37-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.36-py3-none-any.whl (7.7 MB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai-like to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of llama-index-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached llama_index_core-0.12.35-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.34.post1-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.34-py3-none-any.whl (7.6 MB)\n",
      "  Using cached llama_index_core-0.12.33.post1-py3-none-any.whl (7.7 MB)\n",
      "  Using cached llama_index_core-0.12.33-py3-none-any.whl (814 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_index_core-0.12.32-py3-none-any.whl (809 kB)\n",
      "  Using cached llama_index_core-0.12.31-py3-none-any.whl (808 kB)\n",
      "  Using cached llama_index_core-0.12.30-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.29-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.28-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.27-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.26-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.25-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.24.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.24-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.23-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.22-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.21-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.20-py3-none-any.whl (790 kB)\n",
      "  Using cached llama_index_core-0.12.19-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.18-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.17-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.16.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.16-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.15-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.14-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.13-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.12-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.11-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.10-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.9-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.8-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.7-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.6-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.5-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.4-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.3-py3-none-any.whl (1.6 MB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.0\n",
      "  Using cached llama_index_core-0.12.2-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.1-py3-none-any.whl (1.6 MB)\n",
      "  Using cached llama_index_core-0.12.0-py3-none-any.whl (1.6 MB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-llms-openllm\n",
      "  Using cached llama_index_llms_openllm-0.4.1-py3-none-any.whl (2.7 kB)\n",
      "Collecting llama-index-llms-openai-like<0.4.0,>=0.3.1\n",
      "  Using cached llama_index_llms_openai_like-0.3.5-py3-none-any.whl (4.6 kB)\n",
      "Collecting llama-index-llms-openai<0.4,>=0.3.42\n",
      "  Using cached llama_index_llms_openai-0.3.44-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: openai<2,>=1.81.0 in ./.venv/lib/python3.10/site-packages (from llama-index-llms-openai<0.4,>=0.3.42->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-openllm) (1.86.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.2.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-openllm) (3.18.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-openllm) (0.21.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (3.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.73.0->openllm>=0.6.0->llama-index-llms-openllm) (1.3.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-openllm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2>=3.0.1->bentoml==1.4.8->openllm>=0.6.0->llama-index-llms-openllm) (3.0.2)\n",
      "Installing collected packages: safetensors, transformers, llama-index-llms-openai, llama-index-llms-openai-like, llama-index-llms-openllm\n",
      "Successfully installed llama-index-llms-openai-0.3.44 llama-index-llms-openai-like-0.3.5 llama-index-llms-openllm-0.4.1 safetensors-0.5.3 transformers-4.52.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-openllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0a8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:45:38:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]\n",
      "Extracting topics [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [00:42<00:00, 21.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:46:34:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [135], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:47:07:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]\n",
      "Extracting topics [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [00:38<00:00, 19.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:48:06:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [162], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:48:39:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 6, num_workers: 4]: 100%|██████████| 6/6 [00:18<00:00,  3.08s/it]\n",
      "Extracting topics [nodes: 6, num_workers: 4]: 100%|██████████| 6/6 [01:00<00:00, 10.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:50:04:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [392], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:50:37:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:12<00:00,  4.13s/it]\n",
      "Extracting topics [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:38<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:51:33:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [242], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:52:06:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 7, num_workers: 4]: 100%|██████████| 7/7 [00:15<00:00,  2.17s/it]\n",
      "Extracting topics [nodes: 7, num_workers: 4]: 100%|██████████| 7/7 [00:54<00:00,  7.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:53:21:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [270], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:53:55:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 1, num_workers: 4]: 100%|██████████| 1/1 [00:13<00:00, 13.33s/it]\n",
      "Extracting topics [nodes: 1, num_workers: 4]: 100%|██████████| 1/1 [00:21<00:00, 21.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:54:34:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [40], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:55:07:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 1, num_workers: 4]: 100%|██████████| 1/1 [00:07<00:00,  7.72s/it]\n",
      "Extracting topics [nodes: 1, num_workers: 4]: 100%|██████████| 1/1 [00:30<00:00, 30.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:55:50:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [38], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:56:22:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:08<00:00,  2.89s/it]\n",
      "Extracting topics [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:24<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:57:01:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [126], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:57:34:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:14<00:00,  4.95s/it]\n",
      "Extracting topics [nodes: 3, num_workers: 4]: 100%|██████████| 3/3 [00:41<00:00, 13.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:58:36:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [199], batch_writes_enabled: True, batch_write_size: 25]\n",
      "2025-06-23 15:59:09:INFO:g.l.i.e.extraction_pipeline:Running extraction pipeline [batch_size: 4, num_workers: 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Extracting propositions [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]\n",
      "Extracting topics [nodes: 2, num_workers: 4]: 100%|██████████| 2/2 [01:49<00:00, 54.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 16:01:13:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 1, job_sizes: [111], batch_writes_enabled: True, batch_write_size: 25]\n",
      "Extraction complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config, IndexingConfig, ExtractionConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs, S3BasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# extracted_docs = FileBasedDocs(\n",
    "#     docs_directory='extracted'\n",
    "# )\n",
    "\n",
    "extracted_docs = S3BasedDocs(\n",
    "    region='eu-central-1',\n",
    "    bucket_name='grap-bucket',\n",
    "    key_prefix='extracted',\n",
    "    s3_encryption_key_id='arn:aws:kms:eu-central-1:266303292076:key/83d74ccf-7758-4e3d-8dc5-c935eecb58ff'\n",
    ")\n",
    "\n",
    "llm = OpenLLM(model = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        is_chat_model=True,\n",
    "        api_key=aws_key,\n",
    "        max_tokens=4096)\n",
    "\n",
    "llm_embed = OpenAILikeEmbedding(model_name = \"text-embedding-3-small-1\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        # is_chat_model=False,\n",
    "        api_key=azure_key,\n",
    "        dimensions=1024\n",
    "        )\n",
    "\n",
    "GraphRAGConfig.extraction_llm = llm\n",
    "GraphRAGConfig.response_llm = llm\n",
    "GraphRAGConfig.embed_model = llm_embed\n",
    "#GraphRAGConfig.embed_dimensions = 1024\n",
    "\n",
    "checkpoint = Checkpoint('extraction-checkpoint-2')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(graph_store)\n",
    "vector_store = VectorStoreFactory.for_vector_store(vector_store)\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    indexing_config = IndexingConfig(\n",
    "        # chunking=None,\n",
    "        # extraction=ExtractionConfig(\n",
    "            # enable_proposition_extraction = True,\n",
    "            # preferred_entity_classifications = [],\n",
    "            # infer_entity_classifications = False,\n",
    "            # extract_propositions_prompt_template = None,\n",
    "            # extract_topics_prompt_template = None\n",
    "        ) \n",
    ")\n",
    "\n",
    "\n",
    "reader = JSONReader(\n",
    "    levels_back=0,             # Set levels back as needed\n",
    "    collapse_length=None,      # Set collapse length as needed\n",
    "    ensure_ascii=False,        # ASCII encoding option\n",
    "    is_jsonl=False,            # Set if input is JSON Lines format\n",
    "    clean_json=True            # Clean up formatting-only lines\n",
    ")\n",
    "\n",
    "# Find all JSON files in the specified directory\n",
    "# json_files = glob.glob(os.path.join(\"experts_small_2\", \"*.json\"))\n",
    "\n",
    "json_files = glob.glob(os.path.join(\"experts_small_2\", \"*.json\"))\n",
    "\n",
    "# Load the data from each JSON file\n",
    "documents = []\n",
    "for json_file in json_files:\n",
    "    documents.extend(reader.load_data(input_file=json_file, extra_info={}))\n",
    "    \n",
    "for document in documents:\n",
    "    graph_index.extract([document], handler=extracted_docs,  checkpoint=checkpoint, show_progress=True)\n",
    "    time.sleep(30)\n",
    "\n",
    "print('Extraction complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30455f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStore(indexes={'chunk': OpenSearchIndex(index_name='chunk', tenant_id=TenantId(value=None), writeable=True, endpoint='https://r8mamuo7hin4k3xihz9a.eu-central-1.aoss.amazonaws.com', dimensions=1024, embed_model=OpenAILikeEmbedding(model_name='text-embedding-3-small-1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x288666f80>, num_workers=None, additional_kwargs={'dimensions': 1024}, api_key='Gr/4gh0w2jGzDCRaMiHnxgaYolke', api_base='https://us.aigw.galileo.roche.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True, dimensions=1024)), 'statement': OpenSearchIndex(index_name='statement', tenant_id=TenantId(value=None), writeable=True, endpoint='https://r8mamuo7hin4k3xihz9a.eu-central-1.aoss.amazonaws.com', dimensions=1024, embed_model=OpenAILikeEmbedding(model_name='text-embedding-3-small-1', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x288666f80>, num_workers=None, additional_kwargs={'dimensions': 1024}, api_key='Gr/4gh0w2jGzDCRaMiHnxgaYolke', api_base='https://us.aigw.galileo.roche.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True, dimensions=1024))})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4deb2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 16:27:39:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 2, job_sizes: [441, 396], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 396/396 [00:00<00:00, 62140.16it/s]\n",
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 441/441 [00:00<00:00, 62891.03it/s]\n",
      "[27a13/*] Retrying query in 5.806738423246783 seconds because it raised ConflictException: An error occurred (ConflictException) when calling the ExecuteQuery operation (reached max retries: 0): Operation failed due to conflicting concurrent operations (please retry), 0 transactions are currently rolling back. [attempt: 1, query: **REDACTED**, parameters: **REDACTED**]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 396/396 [00:00<00:00, 227713.79it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 441/441 [00:00<00:00, 311920.42it/s]\n",
      "Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'virtual key de********379 rate limit exceeded', 'type': 'virtual_key_rate_limit_error', 'param': None, 'code': None}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 16:33:30:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 2, job_sizes: [151, 200], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 151/151 [00:00<00:00, 61608.94it/s]\n",
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 200/200 [00:00<00:00, 61123.64it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 151/151 [00:00<00:00, 246723.76it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 200/200 [00:00<00:00, 378035.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 16:36:32:INFO:g.l.i.b.build_pipeline:Running build pipeline [batch_size: 4, num_workers: 2, job_sizes: [135, 392], batch_writes_enabled: True, batch_write_size: 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 135/135 [00:00<00:00, 57690.38it/s]\n",
      "Building graph [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 392/392 [00:00<00:00, 56128.33it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 135/135 [00:00<00:00, 154623.44it/s]\n",
      "Building vector index [batch_writes_enabled: True, batch_write_size: 25]: 100%|██████████| 392/392 [00:00<00:00, 333029.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config, IndexingConfig, ExtractionConfig\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.indexing.load import FileBasedDocs, S3BasedDocs\n",
    "from graphrag_toolkit.lexical_graph.indexing.build import Checkpoint\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "\n",
    "\n",
    "set_logging_config('INFO')\n",
    "\n",
    "# extracted_docs = FileBasedDocs(\n",
    "#     docs_directory='extracted'\n",
    "# )\n",
    "\n",
    "extracted_docs = S3BasedDocs(\n",
    "    region='eu-central-1',\n",
    "    bucket_name='grap-bucket',\n",
    "    key_prefix='extracted',\n",
    "    collection_id='20250623-154537',\n",
    "    s3_encryption_key_id='arn:aws:kms:eu-central-1:266303292076:key/83d74ccf-7758-4e3d-8dc5-c935eecb58ff'\n",
    ")\n",
    "\n",
    "llm = OpenLLM(model = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        is_chat_model=True,\n",
    "        api_key=aws_key,\n",
    "        max_tokens=4096)\n",
    "\n",
    "llm_embed = OpenAILikeEmbedding(model_name = \"text-embedding-3-small-1\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        # is_chat_model=False,\n",
    "        api_key=azure_key,\n",
    "        dimensions=1024\n",
    "        )\n",
    "\n",
    "GraphRAGConfig.extraction_llm = llm\n",
    "GraphRAGConfig.response_llm = llm\n",
    "GraphRAGConfig.embed_model = llm_embed\n",
    "#GraphRAGConfig.embed_dimensions = 1024\n",
    "\n",
    "checkpoint = Checkpoint('build-checkpoint-2')\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(graph_store)\n",
    "vector_store = VectorStoreFactory.for_vector_store(vector_store)\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store,\n",
    "    indexing_config = IndexingConfig(\n",
    "        # chunking=None,\n",
    "        # extraction=ExtractionConfig(\n",
    "            # enable_proposition_extraction = True,\n",
    "            # preferred_entity_classifications = [],\n",
    "            # infer_entity_classifications = False,\n",
    "            # extract_propositions_prompt_template = None,\n",
    "            # extract_topics_prompt_template = None\n",
    "        ) \n",
    ")\n",
    "\n",
    "\n",
    "reader = JSONReader(\n",
    "    levels_back=0,             # Set levels back as needed\n",
    "    collapse_length=None,      # Set collapse length as needed\n",
    "    ensure_ascii=False,        # ASCII encoding option\n",
    "    is_jsonl=False,            # Set if input is JSON Lines format\n",
    "    clean_json=True            # Clean up formatting-only lines\n",
    ")\n",
    "\n",
    "# Find all JSON files in the specified directory\n",
    "# json_files = glob.glob(os.path.join(\"experts_small_2\", \"*.json\"))\n",
    "\n",
    "# # Load the data from each JSON file\n",
    "# documents = []\n",
    "# for json_file in json_files:\n",
    "#     documents.extend(reader.load_data(input_file=json_file, extra_info={}))\n",
    "\n",
    "# graph_index.extract(documents, handler=extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "graph_index.build(extracted_docs, checkpoint=checkpoint, show_progress=True)\n",
    "\n",
    "# collection_id = extracted_docs.collection_id\n",
    "\n",
    "print('Build complete')\n",
    "# print(f'collection_id: {collection_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf7ed0",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33c7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, Alex Kelly has skills in Patient Outcomes [source_2.1]. The search results also indicate that this is a recognized skill within the organization, as it appears in the general skills listing [source_1.1].\n"
     ]
    }
   ],
   "source": [
    "from graphrag_toolkit.lexical_graph import LexicalGraphQueryEngine\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from graphrag_toolkit.lexical_graph import GraphRAGConfig\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "\n",
    "llm = OpenLLM(model = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        is_chat_model=True,\n",
    "        api_key=aws_key,\n",
    "        max_tokens=4096)\n",
    "\n",
    "llm_embed = OpenAILikeEmbedding(model_name = \"text-embedding-3-small-1\",\n",
    "        api_base=\"https://us.aigw.galileo.roche.com/v1\",\n",
    "        # is_chat_model=False,\n",
    "        api_key=azure_key,\n",
    "        dimensions=1024\n",
    "        )\n",
    "\n",
    "GraphRAGConfig.extraction_llm = llm\n",
    "GraphRAGConfig.response_llm = llm\n",
    "GraphRAGConfig.embed_model = llm_embed\n",
    "\n",
    "def run_query():\n",
    "        query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(graph_store, vector_store)\n",
    "\n",
    "        response = query_engine.query('''Who has \"Patient Outcomes\" skill?''')\n",
    "\n",
    "        print(response.response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fd4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graph-notebook\n",
      "  Using cached graph_notebook-5.0.1-py3-none-any.whl (21.4 MB)\n",
      "Collecting gremlinpython<=3.7.2,>=3.5.1\n",
      "  Using cached gremlinpython-3.7.2-py2.py3-none-any.whl (78 kB)\n",
      "Collecting ipywidgets<9.0.0,>=8.0.0\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Collecting nbconvert<=7.2.8,>=6.3.0\n",
      "  Using cached nbconvert-7.2.8-py3-none-any.whl (274 kB)\n",
      "Collecting jupyterlab-widgets<4.0.0,>=3.0.0\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Collecting itables<=2.1.0,>=2.0.0\n",
      "  Using cached itables-2.1.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting nbclassic>=1.3.0\n",
      "  Using cached nbclassic-1.3.1-py3-none-any.whl (26.2 MB)\n",
      "Collecting jupyterlab<5.0.0,>=4.3.5\n",
      "  Using cached jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
      "Collecting requests<=2.32.2,>=2.32.0\n",
      "  Using cached requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: nest-asyncio<=1.6.0,>=1.5.5 in ./.venv/lib/python3.10/site-packages (from graph-notebook) (1.6.0)\n",
      "Collecting jupyter-server-proxy<5.0.0,>=4.0.0\n",
      "  Using cached jupyter_server_proxy-4.4.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: botocore>=1.34.74 in ./.venv/lib/python3.10/site-packages (from graph-notebook) (1.36.1)\n",
      "Collecting jupyter-server<3.0.0,>=2.0.0\n",
      "  Using cached jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "Requirement already satisfied: jupyter-client<9.0.0,>=8.0.0 in ./.venv/lib/python3.10/site-packages (from graph-notebook) (8.6.3)\n",
      "Collecting nbclient>=0.7.3\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting neo4j<=5.23.1,>=5.0.0\n",
      "  Using cached neo4j-5.23.1-py3-none-any.whl (293 kB)\n",
      "Collecting sparqlwrapper==2.0.0\n",
      "  Using cached SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Collecting notebook<8.0.0,>=7.0.0\n",
      "  Using cached notebook-7.4.3-py3-none-any.whl (14.3 MB)\n",
      "Collecting json-repair==0.29.2\n",
      "  Using cached json_repair-0.29.2-py3-none-any.whl (15 kB)\n",
      "Collecting numba==0.60.0\n",
      "  Using cached numba-0.60.0-cp310-cp310-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in ./.venv/lib/python3.10/site-packages (from graph-notebook) (6.29.5)\n",
      "Collecting numpy<1.24.0,>=1.23.5\n",
      "  Using cached numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "Collecting jinja2<=3.1.4,>=3.0.3\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting pandas<=2.2.2,>=2.1.0\n",
      "  Using cached pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Collecting networkx==2.4\n",
      "  Using cached networkx-2.4-py3-none-any.whl (1.6 MB)\n",
      "Collecting ipython<=8.10.0,>=7.16.1\n",
      "  Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting ipyfilechooser==0.6.0\n",
      "  Using cached ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting rdflib==7.0.0\n",
      "  Using cached rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "Requirement already satisfied: boto3>=1.34.74 in ./.venv/lib/python3.10/site-packages (from graph-notebook) (1.36.1)\n",
      "Collecting jedi<=0.18.2,>=0.18.1\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.10/site-packages (from networkx==2.4->graph-notebook) (5.2.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-macosx_11_0_arm64.whl (28.8 MB)\n",
      "Collecting pyparsing<4,>=2.1.0\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in ./.venv/lib/python3.10/site-packages (from boto3>=1.34.74->graph-notebook) (0.11.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.venv/lib/python3.10/site-packages (from boto3>=1.34.74->graph-notebook) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.venv/lib/python3.10/site-packages (from botocore>=1.34.74->graph-notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./.venv/lib/python3.10/site-packages (from botocore>=1.34.74->graph-notebook) (2.4.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in ./.venv/lib/python3.10/site-packages (from gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (1.17.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in ./.venv/lib/python3.10/site-packages (from gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (3.12.12)\n",
      "Collecting aenum<4.0.0,>=1.4.5\n",
      "  Using cached aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (5.8.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (0.2.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (24.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (5.14.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (1.8.14)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (0.1.7)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (6.5.1)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->graph-notebook) (0.1.4)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in ./.venv/lib/python3.10/site-packages (from ipython<=8.10.0,>=7.16.1->graph-notebook) (3.0.51)\n",
      "Collecting backcall\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pickleshare\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython<=8.10.0,>=7.16.1->graph-notebook) (2.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython<=8.10.0,>=7.16.1->graph-notebook) (4.9.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython<=8.10.0,>=7.16.1->graph-notebook) (0.6.3)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.venv/lib/python3.10/site-packages (from jedi<=0.18.2,>=0.18.1->graph-notebook) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2<=3.1.4,>=3.0.3->graph-notebook) (3.0.2)\n",
      "Collecting overrides>=5.0\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./.venv/lib/python3.10/site-packages (from jupyter-server<3.0.0,>=2.0.0->graph-notebook) (1.8.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in ./.venv/lib/python3.10/site-packages (from jupyter-server<3.0.0,>=2.0.0->graph-notebook) (4.9.0)\n",
      "Collecting prometheus-client>=0.9\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Collecting jupyter-events>=0.11.0\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Collecting simpervisor>=1.0.0\n",
      "  Using cached simpervisor-1.0.0-py3-none-any.whl (8.3 kB)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in ./.venv/lib/python3.10/site-packages (from jupyterlab<5.0.0,>=4.3.5->graph-notebook) (0.28.1)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./.venv/lib/python3.10/site-packages (from jupyterlab<5.0.0,>=4.3.5->graph-notebook) (63.2.0)\n",
      "Collecting tomli>=1.2.2\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Collecting notebook-shim>=0.2\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Collecting ipython-genutils\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from nbconvert<=7.2.8,>=6.3.0->graph-notebook) (4.13.4)\n",
      "Collecting tinycss2\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting mistune<3,>=2.0.3\n",
      "  Using cached mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: defusedxml in ./.venv/lib/python3.10/site-packages (from nbconvert<=7.2.8,>=6.3.0->graph-notebook) (0.7.1)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pytz in ./.venv/lib/python3.10/site-packages (from neo4j<=5.23.1,>=5.0.0->graph-notebook) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas<=2.2.2,>=2.1.0->graph-notebook) (2025.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<=2.32.2,>=2.32.0->graph-notebook) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<=2.32.2,>=2.32.0->graph-notebook) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<=2.32.2,>=2.32.0->graph-notebook) (3.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (6.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython<=3.7.2,>=3.5.1->graph-notebook) (1.20.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3.0.0,>=2.0.0->graph-notebook) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in ./.venv/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3.0.0,>=2.0.0->graph-notebook) (4.13.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3.0.0,>=2.0.0->graph-notebook) (1.3.1)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<5.0.0,>=4.3.5->graph-notebook) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<5.0.0,>=4.3.5->graph-notebook) (0.16.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.5.0->graph-notebook) (4.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.3 in ./.venv/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3.0.0,>=2.0.0->graph-notebook) (6.0.2)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting jsonschema[format-nongpl]>=4.18.0\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting referencing\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting json5>=0.9.0\n",
      "  Using cached json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting babel>=2.10\n",
      "  Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython<=8.10.0,>=7.16.1->graph-notebook) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython<=8.10.0,>=7.16.1->graph-notebook) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->nbconvert<=7.2.8,>=6.3.0->graph-notebook) (2.7)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<=8.10.0,>=7.16.1->graph-notebook) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<=8.10.0,>=7.16.1->graph-notebook) (2.2.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<=8.10.0,>=7.16.1->graph-notebook) (0.2.3)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.25.1-cp310-cp310-macosx_11_0_arm64.whl (358 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./.venv/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3.0.0,>=2.0.0->graph-notebook) (3.0.0)\n",
      "Collecting uri-template\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Using cached cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl (178 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Collecting types-python-dateutil>=2.8.10\n",
      "  Using cached types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: webencodings, pickleshare, mistune, ipython-genutils, fastjsonschema, backcall, aenum, widgetsnbextension, webcolors, uri-template, types-python-dateutil, tomli, tinycss2, terminado, simpervisor, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, requests, python-json-logger, pyparsing, pycparser, prometheus-client, pandocfilters, overrides, numpy, networkx, neo4j, llvmlite, jupyterlab-widgets, jupyterlab-pygments, json5, json-repair, jinja2, jedi, isodate, fqdn, bleach, babel, async-timeout, async-lru, referencing, rdflib, pandas, numba, jupyter-server-terminals, ipython, cffi, arrow, sparqlwrapper, jsonschema-specifications, itables, isoduration, ipywidgets, argon2-cffi-bindings, jsonschema, ipyfilechooser, gremlinpython, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-server-proxy, jupyter-lsp, nbclassic, jupyterlab, notebook, graph-notebook\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.2\n",
      "    Uninstalling jedi-0.19.2:\n",
      "      Successfully uninstalled jedi-0.19.2\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.0\n",
      "    Uninstalling pandas-2.3.0:\n",
      "      Successfully uninstalled pandas-2.3.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.37.0\n",
      "    Uninstalling ipython-8.37.0:\n",
      "      Successfully uninstalled ipython-8.37.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-core 0.12.37 requires networkx>=3.0, but you have networkx 2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aenum-3.1.16 argon2-cffi-25.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 async-timeout-4.0.3 babel-2.17.0 backcall-0.2.0 bleach-6.2.0 cffi-1.17.1 fastjsonschema-2.21.1 fqdn-1.5.1 graph-notebook-5.0.1 gremlinpython-3.7.2 ipyfilechooser-0.6.0 ipython-8.10.0 ipython-genutils-0.2.0 ipywidgets-8.1.7 isodate-0.6.1 isoduration-20.11.0 itables-2.1.0 jedi-0.18.2 jinja2-3.1.4 json-repair-0.29.2 json5-0.12.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-proxy-4.4.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab-widgets-3.0.15 llvmlite-0.43.0 mistune-2.0.5 nbclassic-1.3.1 nbclient-0.10.2 nbconvert-7.2.8 nbformat-5.10.4 neo4j-5.23.1 networkx-2.4 notebook-7.4.3 notebook-shim-0.2.4 numba-0.60.0 numpy-1.23.5 overrides-7.7.0 pandas-2.2.2 pandocfilters-1.5.1 pickleshare-0.7.5 prometheus-client-0.22.1 pycparser-2.22 pyparsing-3.2.3 python-json-logger-3.3.0 rdflib-7.0.0 referencing-0.36.2 requests-2.32.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.25.1 send2trash-1.8.3 simpervisor-1.0.0 sparqlwrapper-2.0.0 terminado-0.18.1 tinycss2-1.4.0 tomli-2.2.1 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graph-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8eec7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set notebook config to:\n",
      "{\n",
      "    \"host\": \"g-76pgb4ql43.eu-central-1.neptune-graph.amazonaws.com\",\n",
      "    \"neptune_service\": \"neptune-graph\",\n",
      "    \"port\": 443,\n",
      "    \"proxy_host\": \"\",\n",
      "    \"proxy_port\": 8182,\n",
      "    \"auth_mode\": \"IAM\",\n",
      "    \"load_from_s3_arn\": \"arn:aws:s3:::881490108440-handy-bucket\",\n",
      "    \"ssl\": true,\n",
      "    \"ssl_verify\": true,\n",
      "    \"aws_region\": \"eu-central-1\",\n",
      "    \"sparql\": {\n",
      "        \"path\": \"\"\n",
      "    },\n",
      "    \"gremlin\": {\n",
      "        \"connection_protocol\": \"http\",\n",
      "        \"traversal_source\": \"g\",\n",
      "        \"username\": \"\",\n",
      "        \"password\": \"\",\n",
      "        \"message_serializer\": \"GraphSONUntypedMessageSerializerV4\"\n",
      "    },\n",
      "    \"neo4j\": {\n",
      "        \"username\": \"neo4j\",\n",
      "        \"password\": \"password\",\n",
      "        \"auth\": true,\n",
      "        \"database\": null\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graph_notebook.configuration.generate_config.Configuration at 0x168165090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%graph_notebook_config\n",
    "{\n",
    "  \"host\": \"g-76pgb4ql43.eu-central-1.neptune-graph.amazonaws.com\",\n",
    "  \"neptune_service\": \"neptune-graph\",\n",
    "  \"port\": 443,\n",
    "  \"auth_mode\": \"IAM\",\n",
    "  \"load_from_s3_arn\": \"arn:aws:s3:::881490108440-handy-bucket\",\n",
    "  \"ssl\": true,\n",
    "  \"ssl_verify\": true,\n",
    "  \"aws_region\": \"eu-central-1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563eb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a valid configuration. Do not forget to validate your settings using %graph_notebook_config.\n"
     ]
    }
   ],
   "source": [
    "%load_ext graph_notebook.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2710afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, several individuals speak English:\n",
      "\n",
      "1. Perkins speaks English at Fluent proficiency, though it is not their native language [source_1]\n",
      "\n",
      "2. Beth Roman speaks English fluently [source_2]\n",
      "\n",
      "3. An unnamed employee possesses English Language skills at Native proficiency, and English is their native language [source_3]\n",
      "\n",
      "4. Donald Flynn speaks English at a Fluent proficiency level, though it is not his native language [source_4]\n",
      "\n",
      "5. Alex Kelly speaks English at Fluent proficiency, and English is their native language [source_5]\n"
     ]
    }
   ],
   "source": [
    "query_engine = LexicalGraphQueryEngine.for_semantic_guided_search(graph_store, vector_store)\n",
    "response = query_engine.query(''' Who speaks ENGLISH? ''')\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cec5137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find valid IAM credentials in any the following locations:\n",
      "\n",
      "env, assume-role, assume-role-with-web-identity, sso, shared-credential-file, custom-process, config-file, ec2-credentials-file, boto-config, container-role, iam-role\n",
      "\n",
      "Go to https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for more details on configuring your IAM credentials.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4c106c8dd4e6380b36d875a85847c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(layout=Layout(overflow='scroll')),), selected_index=0, titles=('Error',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%oc\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4285d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aenum==3.1.16\n",
      "aioboto3==13.4.0\n",
      "aiobotocore==2.18.0\n",
      "aiofiles==24.1.0\n",
      "aiohappyeyeballs==2.6.1\n",
      "aiohttp==3.12.12\n",
      "aioitertools==0.12.0\n",
      "aiosignal==1.3.2\n",
      "aiosqlite==0.21.0\n",
      "annotated-types==0.7.0\n",
      "anthropic==0.54.0\n",
      "anthropic-bedrock==0.8.0\n",
      "anyio==4.9.0\n",
      "appnope==0.1.4\n",
      "argon2-cffi==25.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==3.0.0\n",
      "async-lru==2.0.5\n",
      "async-timeout==4.0.3\n",
      "attrs==25.3.0\n",
      "babel==2.17.0\n",
      "backcall==0.2.0\n",
      "banks==2.1.2\n",
      "beautifulsoup4==4.13.4\n",
      "bleach==6.2.0\n",
      "blis==0.7.11\n",
      "boto3==1.36.1\n",
      "botocore==1.36.1\n",
      "cachetools==5.5.2\n",
      "catalogue==2.0.10\n",
      "certifi==2025.4.26\n",
      "cffi==1.17.1\n",
      "charset-normalizer==3.4.2\n",
      "chromedriver-autoinstaller==0.6.4\n",
      "click==8.2.1\n",
      "cloudpathlib==0.21.1\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "confection==0.1.5\n",
      "cssselect==1.3.0\n",
      "cymem==2.0.11\n",
      "dataclasses-json==0.6.7\n",
      "debugpy==1.8.14\n",
      "decorator==5.2.1\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.18\n",
      "dirtyjson==1.0.8\n",
      "distro==1.9.0\n",
      "Events==0.5\n",
      "exceptiongroup==1.3.0\n",
      "executing==2.2.0\n",
      "fastjsonschema==2.21.1\n",
      "feedfinder2==0.0.4\n",
      "feedparser==6.0.11\n",
      "filelock==3.18.0\n",
      "filetype==1.2.0\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.7.0\n",
      "fsspec==2025.5.1\n",
      "google-auth==2.40.3\n",
      "graph-notebook==5.0.1\n",
      "graphrag-toolkit-lexical-graph @ https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.8.1.zip#subdirectory=lexical-graph\n",
      "greenlet==3.2.3\n",
      "gremlinpython==3.7.2\n",
      "griffe==1.7.3\n",
      "h11==0.16.0\n",
      "hf-xet==1.1.3\n",
      "html2text==2024.2.26\n",
      "httpcore==1.0.9\n",
      "httpx==0.28.1\n",
      "huggingface-hub==0.33.0\n",
      "idna==3.10\n",
      "ipyfilechooser==0.6.0\n",
      "ipykernel==6.29.5\n",
      "ipython==8.10.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.1.7\n",
      "isodate==0.6.1\n",
      "isoduration==20.11.0\n",
      "itables==2.1.0\n",
      "jedi==0.18.2\n",
      "jieba3k==0.35.1\n",
      "Jinja2==3.1.4\n",
      "jiter==0.10.0\n",
      "jmespath==1.0.1\n",
      "joblib==1.5.1\n",
      "json2xml==5.0.5\n",
      "json5==0.12.0\n",
      "json_repair==0.29.2\n",
      "jsonpatch==1.33\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.24.0\n",
      "jsonschema-specifications==2025.4.1\n",
      "jupyter-events==0.12.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.8.1\n",
      "jupyter_server==2.16.0\n",
      "jupyter_server_proxy==4.4.0\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.4.3\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.15\n",
      "langchain-core==0.3.65\n",
      "langchain-openai==0.3.22\n",
      "langcodes==3.5.0\n",
      "langsmith==0.3.45\n",
      "language_data==1.3.0\n",
      "llama-index-core==0.12.37\n",
      "llama-index-embeddings-bedrock==0.5.0\n",
      "llama-index-embeddings-openai==0.3.1\n",
      "llama-index-embeddings-openai-like==0.1.1\n",
      "llama-index-llms-anthropic==0.6.19\n",
      "llama-index-llms-bedrock-converse==0.6.0\n",
      "llama-index-llms-openai==0.3.44\n",
      "llama-index-llms-openai-like==0.3.5\n",
      "llama-index-llms-openllm==0.4.1\n",
      "llama-index-readers-json==0.3.0\n",
      "llama-index-readers-web==0.4.2\n",
      "llama-index-retrievers-bedrock==0.3.0\n",
      "llama-index-vector-stores-opensearch==0.5.6\n",
      "llvmlite==0.43.0\n",
      "lru-dict==1.3.0\n",
      "lxml==5.4.0\n",
      "lxml_html_clean==0.4.2\n",
      "marisa-trie==1.2.1\n",
      "markdown-it-py==3.0.0\n",
      "markdownify==1.1.0\n",
      "MarkupSafe==3.0.2\n",
      "marshmallow==3.26.1\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==2.0.5\n",
      "multidict==6.4.4\n",
      "murmurhash==1.0.13\n",
      "mypy_extensions==1.1.0\n",
      "nbclassic==1.3.1\n",
      "nbclient==0.10.2\n",
      "nbconvert==7.2.8\n",
      "nbformat==5.10.4\n",
      "neo4j==5.23.1\n",
      "nest-asyncio==1.6.0\n",
      "networkx==2.4\n",
      "newspaper3k==0.2.8\n",
      "nltk==3.9.1\n",
      "notebook==7.4.3\n",
      "notebook_shim==0.2.4\n",
      "numba==0.60.0\n",
      "numpy==1.23.5\n",
      "openai==1.86.0\n",
      "opensearch-py==2.8.0\n",
      "orjson==3.10.18\n",
      "outcome==1.3.0.post0\n",
      "overrides==7.7.0\n",
      "oxylabs==2.0.0\n",
      "packaging==24.2\n",
      "pandas==2.2.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "pgvector==0.4.1\n",
      "pickleshare==0.7.5\n",
      "pillow==11.2.1\n",
      "pipe==2.2\n",
      "platformdirs==4.3.8\n",
      "playwright==1.52.0\n",
      "preshed==3.0.10\n",
      "prometheus_client==0.22.1\n",
      "prompt_toolkit==3.0.51\n",
      "propcache==0.3.2\n",
      "psutil==7.0.0\n",
      "psycopg2-binary==2.9.10\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.2\n",
      "pycparser==2.22\n",
      "pydantic==2.11.5\n",
      "pydantic_core==2.33.2\n",
      "pyee==13.0.0\n",
      "Pygments==2.19.1\n",
      "pyparsing==3.2.3\n",
      "PySocks==1.7.1\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.1\n",
      "python-json-logger==3.3.0\n",
      "pytz==2025.2\n",
      "PyYAML==6.0.2\n",
      "pyzmq==26.4.0\n",
      "rdflib==7.0.0\n",
      "referencing==0.36.2\n",
      "regex==2024.11.6\n",
      "requests==2.32.2\n",
      "requests-file==2.1.0\n",
      "requests-toolbelt==1.0.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==14.0.0\n",
      "rpds-py==0.25.1\n",
      "rsa==4.9.1\n",
      "s3transfer==0.11.3\n",
      "safetensors==0.5.3\n",
      "scikit-learn==1.7.0\n",
      "scipy==1.15.3\n",
      "selenium==4.33.0\n",
      "Send2Trash==1.8.3\n",
      "sgmllib3k==1.0.0\n",
      "shellingham==1.5.4\n",
      "simpervisor==1.0.0\n",
      "six==1.17.0\n",
      "smart-open==7.1.0\n",
      "sniffio==1.3.1\n",
      "sortedcontainers==2.4.0\n",
      "soupsieve==2.7\n",
      "spacy==3.7.5\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.5\n",
      "SPARQLWrapper==2.0.0\n",
      "spider-client==0.0.27\n",
      "SQLAlchemy==2.0.41\n",
      "srsly==2.5.1\n",
      "stack-data==0.6.3\n",
      "tenacity==9.1.2\n",
      "terminado==0.18.1\n",
      "tfidf-matcher==0.3.0\n",
      "thinc==8.2.5\n",
      "threadpoolctl==3.6.0\n",
      "tiktoken==0.9.0\n",
      "tinycss2==1.4.0\n",
      "tinysegmenter==0.3\n",
      "tldextract==5.3.0\n",
      "tokenizers==0.21.1\n",
      "tomli==2.2.1\n",
      "tornado==6.5.1\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "transformers==4.52.4\n",
      "trio==0.30.0\n",
      "trio-websocket==0.12.2\n",
      "typer==0.16.0\n",
      "types-python-dateutil==2.9.0.20250516\n",
      "typing-inspect==0.9.0\n",
      "typing-inspection==0.4.1\n",
      "typing_extensions==4.13.2\n",
      "tzdata==2025.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.4.0\n",
      "wasabi==1.1.3\n",
      "wcwidth==0.2.13\n",
      "weasel==0.4.1\n",
      "webcolors==24.11.1\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "widgetsnbextension==4.0.14\n",
      "wrapt==1.17.2\n",
      "wsproto==1.2.0\n",
      "yarl==1.20.1\n",
      "zstandard==0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
